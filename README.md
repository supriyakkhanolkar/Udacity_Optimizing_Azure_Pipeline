# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
This dataset contains data about customers who are contacted to market banking products. We seek to predict if the customer will buy the product. The dataset has 21 columns out of which 20 are used as input features and 1 target column. Input features include information about customer such as age, job type, marital status, education, housing, information about contacts made with customer, outcome of previous campaign etc. Target column indicates if the customer has subscribed to term deposit. Possible values of target column are yes / no.

We use 2 approaches to arrive at a solution. The first approach uses Hyperdrive to get best values of hyperparameters for Logistic Regression algorithm. The second approach uses AutoML to get the best model for the same dataset.

The best performing model was Scikit-learn LogisticRegression using Hyperdrive with accuracy of 0.9159462939120838

## Scikit-learn Pipeline
First we create a training script train.pyipynb to take care of following:
*	connect to a web based database to create a tabular dataset
*	clean the dataset
*	split the dataset into test and train data
*	Call scikit-learn LogisticRegression using given values of Regularization Strength and Max Iterations
*	Create model file

Then we create a notebook udacity-project.ipynb to take care of following:
*	Create compute cluster
*	Use HyperDrive to tune hyperparameters using hyper drive config that supplies training script along with other parameters
*	Find out the best model created by HyperDrive and register it

We chose Random Parameter Sampling for following reaasons:
*	It supports discrete as well as continuous hyperparameters
*	It supports early termination of low performance runs


We chose Median Stopping Policy for early stopping. It is an early stopping policy based on running averages of primary metrics reported by the runs. This policy computes running averages across all training runs and terminates runs with primary metric values worse than the median of averages.


## AutoML
We use the same notebook udacity-project.ipynb to take care of following:
*	Create a tabular dataset using the same database as used by earlier experiment
*	clean the dataset
*	split the dataset into test and train data
*	Create AutoML experiment by providing automl config parameters and submit it
*	We find the optimized model prepared by AutoML and register it

We delete the compute cluster in the end.

## Pipeline comparison
Results using Logistic Regression with HyperDrive
-------------------------------------------------
We got following results for best metric with Logistic Regression using HyperDrive :
Accuracy = 0.9159462939120838
For a comination with 
Regularization Strength = 2.078306649243789, 
Max Iterations = 82
We observed that out of 20 runs of HyperDrive, 17 runs got accuracy close to 0.915 for varying values of Regularization Strength and Max Iterations.

Results using Automated ML
--------------------------
We got following results for best metric using AutoML :
Accuracy = 0.91439
Model : VotingEnsemble

The parameters generated by VotingEnsamble model in our case are as follows:
reg_lambda=0.6842105263157894,                                                                                                                                                            subsample=0.5942105263157895,                                                                                            subsample_for_bin=200000,                                                                                                subsample_freq=0

Comparison
----------
As we can see that the difference between both approaches is less than 1%. Approach using Logistic Regression with Hyperdrive is marginally better than approach using AutoML in this case.

Logistic Regression is easy to implement and efficient to train. It makes no assumptions about distribution of classes in feature space. It provides good accuracy for simple datasets. It is less inclined to overfitting, but it can overfit in high dimensional datasets. Also, it is tough to obtain complex relationships using Logistic Regression.

VotingEnsamble is an ensamble learning model. Creating ensembles can improve machine learning results by combining multiple iterations that may provide better predictions compared to a single iteration. VotingEnsamble is applicable to classification as well as regression. In case of classification it uses weighted average of predicted class probabilities. In case of regression it uses weighted average of predicted regression targets. Our current task needs prediction using classification.
VotingEnsamble in Azure AutoML uses soft voting which provides prediction probability as weighted average of prediction probabilities of each model used in ensamble. 

## Future work
When using Hyperdrive we can do following improvements:
* Do an initial search with random sampling and then refine the search space to improve results
* Use Bayesian sampling

Also, we can make use of model explanations produced by AutoML run to find out top K features that affect the result.

